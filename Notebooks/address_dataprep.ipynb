{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polars version: 1.9.0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import string\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "print(f'polars version: {pl.__version__}')\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"geocoder_llm_project\", timeout=300)\n",
    "\n",
    "project_dir = Path(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://nationaladdressdata.s3.amazonaws.com/NAD_r18_TXT.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip_data_file_path = project_dir / 'NAD_r18_TXT.zip'\n",
    "\n",
    "# with zipfile.ZipFile(zip_data_file_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file_path = project_dir / 'TXT/NAD_r18.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset schema. Everything is a string here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_overrides = {\n",
    "    \"OID_\": pl.Int64,\n",
    "    \"AddNum_Pre\": pl.Utf8,\n",
    "    \"Add_Number\": pl.Int64,\n",
    "    \"AddNum_Suf\": pl.Utf8,\n",
    "    \"AddNo_Full\": pl.Int64,\n",
    "    \"St_PreMod\": pl.Utf8,\n",
    "    \"St_PreDir\": pl.Utf8,\n",
    "    \"St_PreTyp\": pl.Utf8,\n",
    "    \"St_PreSep\": pl.Utf8,\n",
    "    \"St_Name\": pl.Utf8,\n",
    "    \"St_PosTyp\": pl.Utf8,\n",
    "    \"St_PosDir\": pl.Utf8,\n",
    "    \"St_PosMod\": pl.Utf8,\n",
    "    \"StNam_Full\": pl.Utf8,\n",
    "    \"Building\": pl.Utf8,\n",
    "    \"Floor\": pl.Utf8,\n",
    "    \"Unit\": pl.Utf8,\n",
    "    \"Room\": pl.Utf8,\n",
    "    \"Seat\": pl.Utf8,\n",
    "    \"Addtl_Loc\": pl.Utf8,\n",
    "    \"SubAddress\": pl.Utf8,\n",
    "    \"LandmkName\": pl.Utf8,\n",
    "    \"County\": pl.Utf8,\n",
    "    \"Inc_Muni\": pl.Utf8,\n",
    "    \"Post_City\": pl.Utf8,\n",
    "    \"Census_Plc\": pl.Utf8,\n",
    "    \"Uninc_Comm\": pl.Utf8,\n",
    "    \"Nbrhd_Comm\": pl.Utf8,\n",
    "    \"NatAmArea\": pl.Utf8,\n",
    "    \"NatAmSub\": pl.Utf8,\n",
    "    \"Urbnztn_PR\": pl.Utf8,\n",
    "    \"PlaceOther\": pl.Utf8,\n",
    "    \"PlaceNmTyp\": pl.Utf8,\n",
    "    \"State\": pl.Utf8,\n",
    "    \"Zip_Code\": pl.Int64,\n",
    "    \"Plus_4\": pl.Int64,\n",
    "    \"UUID\": pl.Utf8,\n",
    "    \"AddAuth\": pl.Int64,\n",
    "    \"AddrRefSys\": pl.Utf8,\n",
    "    \"Longitude\": pl.Float64,\n",
    "    \"Latitude\": pl.Float64,\n",
    "    \"NatGrid\": pl.Utf8,\n",
    "    \"Elevation\": pl.Utf8,\n",
    "    \"Placement\": pl.Utf8,\n",
    "    \"AddrPoint\": pl.Utf8,\n",
    "    \"Related_ID\": pl.Utf8,\n",
    "    \"RelateType\": pl.Utf8,\n",
    "    \"ParcelSrc\": pl.Utf8,\n",
    "    \"Parcel_ID\": pl.Utf8,\n",
    "    \"AddrClass\": pl.Utf8,\n",
    "    \"Lifecycle\": pl.Utf8,\n",
    "    \"Effective\": pl.Utf8,\n",
    "    \"Expire\": pl.Utf8,\n",
    "    \"DateUpdate\": pl.Utf8,\n",
    "    \"AnomStatus\": pl.Utf8,\n",
    "    \"LocatnDesc\": pl.Utf8,\n",
    "    \"Addr_Type\": pl.Utf8,\n",
    "    \"DeliverTyp\": pl.Utf8,\n",
    "    \"NAD_Source\": pl.Utf8,\n",
    "    \"DataSet_ID\": pl.Utf8,\n",
    "    \"StreetAddress\": pl.Utf8,\n",
    "    \"SecondaryAddress\": pl.Utf8,\n",
    "    \"CityStateZip\": pl.Utf8,\n",
    "    \"FullAddress\": pl.Utf8,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data is huge (31GB) and build over years, there are some inconsistencies. So we ignore errors, infer schema and provide null values as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(\n",
    "    raw_file_path, \n",
    "    ignore_errors=True, \n",
    "    separator=\",\", \n",
    "    infer_schema_length=0, \n",
    "    quote_char=None, \n",
    "    schema_overrides=schema_overrides,\n",
    "    truncate_ragged_lines=True,\n",
    "    null_values=[\"Not stated\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out states which are not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(pl.col('State').is_not_null())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the states available in the data. We have data from 47 states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_states = [\n",
    "    'TX', 'LA', 'ME', 'WY', 'KY', 'MI', 'WA', 'VT', 'ND', 'TN',\n",
    "    'IN', 'WV', 'MN', 'RI', 'DE', 'IL', 'SD', 'AK', 'MS', 'OK',\n",
    "    'PA', 'WI', 'NY', 'KS', 'NM', 'AZ', 'SC', 'FL', 'NC', 'MD',\n",
    "    'UT', 'NE', 'NH', 'VA', 'GA', 'AL', 'CA', 'MA', 'CT', 'AR',\n",
    "    'CO', 'MT', 'DC', 'ID', 'IA', 'OH', 'MO'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(\n",
    "    pl.col('State').is_in(valid_states)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 80044721\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of records: {len(df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatinating different columns into single strings to get street and country information, and finally build the FullAddress column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(\n",
    "    pl.concat_str(\n",
    "        [\n",
    "            pl.col(\"AddNum_Pre\"),\n",
    "            pl.col(\"Add_Number\").cast(str),\n",
    "            pl.col(\"AddNum_Suf\"),\n",
    "            pl.col(\"St_PreMod\"),\n",
    "            pl.col(\"St_PreDir\"),\n",
    "            pl.col(\"St_PreTyp\"),\n",
    "            pl.col(\"St_Name\"),\n",
    "            pl.col(\"St_PosTyp\"),\n",
    "            pl.col(\"St_PosDir\"),\n",
    "            pl.col(\"St_PosMod\"),\n",
    "        ],\n",
    "        separator=\" \",\n",
    "        ignore_nulls=True\n",
    "    ).alias(\"StreetAddress\"),\n",
    "    pl.concat_str(\n",
    "        [\n",
    "            pl.col(\"Building\"),\n",
    "            pl.col(\"Floor\"),\n",
    "            pl.col(\"Unit\"),\n",
    "            pl.col(\"Room\"),\n",
    "            pl.col(\"Seat\"),\n",
    "            pl.col(\"Addtl_Loc\"),\n",
    "            pl.col(\"SubAddress\")\n",
    "        ],\n",
    "        separator=\", \",\n",
    "        ignore_nulls=True\n",
    "    ).alias(\"SecondaryAddress\"),\n",
    "    pl.concat_str(\n",
    "        [\n",
    "            pl.col(\"LandmkName\"),\n",
    "            pl.col(\"County\"),\n",
    "            pl.col(\"Inc_Muni\"),\n",
    "            pl.col(\"Post_City\"),\n",
    "            pl.col(\"State\"),\n",
    "            pl.concat_str(\n",
    "                [pl.col(\"Zip_Code\").cast(str), pl.col(\"Plus_4\").cast(str)],\n",
    "                separator=\"-\",\n",
    "                ignore_nulls=True\n",
    "            )\n",
    "        ],\n",
    "        separator=\", \",\n",
    "        ignore_nulls=True\n",
    "    ).alias(\"CityStateZip\")\n",
    ")\n",
    "\n",
    "df = df.with_columns(\n",
    "    pl.concat_str(\n",
    "        [\n",
    "            pl.col(\"StreetAddress\"),\n",
    "            pl.col(\"SecondaryAddress\"),\n",
    "            pl.col(\"CityStateZip\")\n",
    "        ],\n",
    "        separator=\"\\n\",\n",
    "        ignore_nulls=True\n",
    "    ).alias(\"FullAddress\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some inconsistencies found in the `Inc_Muni` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['UNKN', '250240201300', '510000105900', '**PREVIOUS NAME REMOVED BY FDC. (MAYBERRY COURT)', '631332003700', 'SWWJDU 15-5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Mh Sw' = 'south west', 'Mm 100.8 I95 Sb Hwy', 'Hwy' = 'highway', 'Lti' = '', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_chars = '?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "617 MEXBORO Road\n",
      "\n",
      "Monroe, UNINCORPORATED, FRISCO CITY, AL, 36445\n"
     ]
    }
   ],
   "source": [
    "idx = 10\n",
    "print(df['FullAddress'][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean & Median address length: 41-42 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Address Length: 42.41529813065374 | Median Address Length: 41.0\n"
     ]
    }
   ],
   "source": [
    "mean_add_len = df.with_columns(pl.col('FullAddress').str.len_chars().alias('AddressLength'))['AddressLength'].mean()\n",
    "median_add_len = df.with_columns(pl.col('FullAddress').str.len_chars().alias('AddressLength'))['AddressLength'].median()\n",
    "\n",
    "print(f'Mean Address Length: {mean_add_len} | Median Address Length: {median_add_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State full name and abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_name_abbr_tuples = [\n",
    "    (\"Alabama\", \"AL\"),\n",
    "    (\"Alaska\", \"AK\"),\n",
    "    (\"Arizona\", \"AZ\"),\n",
    "    (\"Arkansas\", \"AR\"),\n",
    "    (\"California\", \"CA\"),\n",
    "    (\"Colorado\", \"CO\"),\n",
    "    (\"Connecticut\", \"CT\"),\n",
    "    (\"Delaware\", \"DE\"),\n",
    "    (\"District of Columbia\", \"DC\"),\n",
    "    (\"Florida\", \"FL\"),\n",
    "    (\"Georgia\", \"GA\"),\n",
    "    (\"Idaho\", \"ID\"),\n",
    "    (\"Illinois\", \"IL\"),\n",
    "    (\"Indiana\", \"IN\"),\n",
    "    (\"Iowa\", \"IA\"),\n",
    "    (\"Kansas\", \"KS\"),\n",
    "    (\"Kentucky\", \"KY\"),\n",
    "    (\"Louisiana\", \"LA\"),\n",
    "    (\"Maine\", \"ME\"),\n",
    "    (\"Maryland\", \"MD\"),\n",
    "    (\"Massachusetts\", \"MA\"),\n",
    "    (\"Michigan\", \"MI\"),\n",
    "    (\"Minnesota\", \"MN\"),\n",
    "    (\"Mississippi\", \"MS\"),\n",
    "    (\"Missouri\", \"MO\"),\n",
    "    (\"Montana\", \"MT\"),\n",
    "    (\"Nebraska\", \"NE\"),\n",
    "    (\"New Hampshire\", \"NH\"),\n",
    "    (\"New Mexico\", \"NM\"),\n",
    "    (\"New York\", \"NY\"),\n",
    "    (\"North Carolina\", \"NC\"),\n",
    "    (\"North Dakota\", \"ND\"),\n",
    "    (\"Ohio\", \"OH\"),\n",
    "    (\"Oklahoma\", \"OK\"),\n",
    "    (\"Pennsylvania\", \"PA\"),\n",
    "    (\"Rhode Island\", \"RI\"),\n",
    "    (\"South Carolina\", \"SC\"),\n",
    "    (\"South Dakota\", \"SD\"),\n",
    "    (\"Tennessee\", \"TN\"),\n",
    "    (\"Texas\", \"TX\"),\n",
    "    (\"Utah\", \"UT\"),\n",
    "    (\"Vermont\", \"VT\"),\n",
    "    (\"Virginia\", \"VA\"),\n",
    "    (\"Washington\", \"WA\"),\n",
    "    (\"West Virginia\", \"WV\"),\n",
    "    (\"Wisconsin\", \"WI\"),\n",
    "    (\"Wyoming\", \"WY\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling 10K addresses for each state, from the entire dataset. We do sampling without replacement as we don't want duplicates in the dataset. So we have a dataset of 470K records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_per_state = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_df(df: pl.DataFrame, state_abv: str, samples: int = address_per_state) -> pl.DataFrame:\n",
    "    state_df = df.filter(pl.col('State') == state_abv)\n",
    "\n",
    "    sample_with_replacement = True if len(state_df) < samples else False\n",
    "\n",
    "    return state_df.sample(n=samples, seed=0, with_replacement=sample_with_replacement, shuffle=True) \n",
    "\n",
    "def build_dataset(df: pl.DataFrame, states: List[Tuple[str, str]]) -> pl.DataFrame:\n",
    "    dfs = [get_state_df(df, state_abv) for state, state_abv in states]\n",
    "    return pl.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = build_dataset(df, state_name_abbr_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 470000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of samples: {len(sampled_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique states: 47\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of unique states: {len(sampled_df[\"State\"].unique())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the dataset into a parquet format because it is a columnar store and compresses efficiently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df.write_parquet(project_dir / 'nad_sample_address.parquet', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to Sequence Dataset prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = Path(os.getcwd()).parent\n",
    "data_dir = project_dir / 'Data'\n",
    "null_values = [\"unkn\", \"unincorporated\", \"unknown\", \"null\", \"nan\", \"null\", \"nill\", \"na\", \"none\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (470000, 64)\n",
      "States: ['NC', 'FL', 'OH', 'KS', 'AK', 'NE', 'SC', 'ND', 'SD', 'TN', 'NM', 'VT', 'CO', 'NY', 'LA', 'AZ', 'IL', 'PA', 'WI', 'CA', 'AL', 'MD', 'ID', 'NH', 'WY', 'MS', 'IN', 'MN', 'UT', 'AR', 'WA', 'WV', 'CT', 'MI', 'ME', 'TX', 'DE', 'IA', 'MT', 'OK', 'RI', 'MO', 'MA', 'GA', 'KY', 'DC', 'VA']\n"
     ]
    }
   ],
   "source": [
    "df = pl.read_parquet(data_dir / 'address_dataset.parquet')\n",
    "print(f'Shape of data: {df.shape}')\n",
    "print(f\"States: {df['State'].unique().to_list()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_mapping = {\n",
    "    'TX': 'Texas',\n",
    "    'LA': 'Louisiana',\n",
    "    'ME': 'Maine',\n",
    "    'WY': 'Wyoming',\n",
    "    'KY': 'Kentucky',\n",
    "    'MI': 'Michigan',\n",
    "    'WA': 'Washington',\n",
    "    'VT': 'Vermont',\n",
    "    'ND': 'North Dakota',\n",
    "    'TN': 'Tennessee',\n",
    "    'IN': 'Indiana',\n",
    "    'WV': 'West Virginia',\n",
    "    'MN': 'Minnesota',\n",
    "    'RI': 'Rhode Island',\n",
    "    'DE': 'Delaware',\n",
    "    'IL': 'Illinois',\n",
    "    'SD': 'South Dakota',\n",
    "    'AK': 'Alaska',\n",
    "    'MS': 'Mississippi',\n",
    "    'OK': 'Oklahoma',\n",
    "    'PA': 'Pennsylvania',\n",
    "    'WI': 'Wisconsin',\n",
    "    'NY': 'New York',\n",
    "    'KS': 'Kansas',\n",
    "    'NM': 'New Mexico',\n",
    "    'AZ': 'Arizona',\n",
    "    'SC': 'South Carolina',\n",
    "    'FL': 'Florida',\n",
    "    'NC': 'North Carolina',\n",
    "    'MD': 'Maryland',\n",
    "    'UT': 'Utah',\n",
    "    'NE': 'Nebraska',\n",
    "    'NH': 'New Hampshire',\n",
    "    'VA': 'Virginia',\n",
    "    'GA': 'Georgia',\n",
    "    'AL': 'Alabama',\n",
    "    'CA': 'California',\n",
    "    'MA': 'Massachusetts',\n",
    "    'CT': 'Connecticut',\n",
    "    'AR': 'Arkansas',\n",
    "    'CO': 'Colorado',\n",
    "    'MT': 'Montana',\n",
    "    'DC': 'District of Columbia',\n",
    "    'ID': 'Idaho',\n",
    "    'IA': 'Iowa',\n",
    "    'OH': 'Ohio',\n",
    "    'MO': 'Missouri'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lower case all the string columns, and then replace the occurances of null value strings with np.nan and the replace them with empty strings ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.select(pl.col(pl.Utf8)).columns:\n",
    "    df = df.with_columns(\n",
    "        pl.col(column).str.to_lowercase().alias(column)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.select(pl.col(pl.Utf8)).columns:\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col(column).is_in(null_values)).then(np.nan).otherwise(pl.col(column)).alias(column)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fill_null('')\n",
    "df = df.fill_nan('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.select(pl.col(pl.Utf8)).columns:\n",
    "    x = df.filter(\n",
    "        pl.col(column) == 'nan'\n",
    "    )\n",
    "    if len(x) > 0:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to build the full address and format it appropriately. I try to follow the address format from geopy's `Nominatim` class. This class connects with the openstreet maps data and provides the latitude, longitude and the full address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_usdot_to_freeform_granular(data: dict, state_map: dict) -> str:\n",
    "    # Custom null-like values to filter\n",
    "    NULL_STRINGS = {\"\", None, \"nan\", \"null\"}\n",
    "\n",
    "    def safe_get(key):\n",
    "        val = data.get(key)\n",
    "        if isinstance(val, str):\n",
    "            val = val.lower()\n",
    "        return None if (val in NULL_STRINGS or str(val).strip() in NULL_STRINGS) else str(val).strip()\n",
    "\n",
    "    def safe_title(key):\n",
    "        val = safe_get(key)\n",
    "        return val.title() if val else None\n",
    "\n",
    "    # House number\n",
    "    number = \" \".join(filter(None, [safe_get(\"AddNum_Pre\"),\n",
    "                                    safe_get(\"Add_Number\"),\n",
    "                                    safe_get(\"AddNum_Suf\")]))\n",
    "\n",
    "    # Street full\n",
    "    street_parts = [\n",
    "        safe_get(\"St_PreDir\"),\n",
    "        safe_title(\"St_Name\"),\n",
    "        safe_title(\"St_PosTyp\"),\n",
    "        safe_get(\"St_PosDir\")\n",
    "    ]\n",
    "    street = \" \".join(part for part in street_parts if part)\n",
    "\n",
    "    # Unit/building details\n",
    "    sub_parts = []\n",
    "    if safe_get(\"Building\"): sub_parts.append(f\"Bldg {safe_get('Building')}\")\n",
    "    if safe_get(\"Floor\"): sub_parts.append(f\"Floor {safe_get('Floor')}\")\n",
    "    if safe_get(\"Unit\"): sub_parts.append(f\"Unit {safe_get('Unit')}\")\n",
    "    if safe_get(\"Room\"): sub_parts.append(f\"Room {safe_get('Room')}\")\n",
    "\n",
    "    sub_address = \", \".join(sub_parts)\n",
    "\n",
    "    # Town/City\n",
    "    town = safe_title(\"Uninc_Comm\") or safe_title(\"Inc_Muni\")\n",
    "\n",
    "    # County\n",
    "    county = safe_title(\"County\")\n",
    "\n",
    "    # State\n",
    "    state_abbr = safe_get(\"State\")\n",
    "    state_full = state_map.get(state_abbr.upper(), state_abbr) if state_abbr else None\n",
    "\n",
    "    # ZIP\n",
    "    zip_raw = safe_get(\"Zip_Code\")\n",
    "    zip_code = zip_raw.zfill(5) if zip_raw and zip_raw.isdigit() else None\n",
    "\n",
    "    # Compose full address\n",
    "    components = [number, street]\n",
    "    if sub_address:\n",
    "        components.append(sub_address)\n",
    "    components.extend([\n",
    "        town,\n",
    "        f\"{county} County\" if county else None,\n",
    "        state_full,\n",
    "        zip_code\n",
    "    ])\n",
    "\n",
    "    return \", \".join([c for c in components if c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_addresses = [\n",
    "    format_usdot_to_freeform_granular(r, state_mapping) \n",
    "    for r in df.rows(named=True)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(\n",
    "    pl.Series(\"FormattedFullAddress\", formatted_addresses)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean and median address lengths have increased to 65 characters now. This new formatting makes the address strings more clear and easier to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Address Length: 64.9411170212766 | Median Address Length: 64.0\n"
     ]
    }
   ],
   "source": [
    "mean_add_len = df.with_columns(pl.col('FormattedFullAddress').str.len_chars().alias('AddressLength'))['AddressLength'].mean()\n",
    "median_add_len = df.with_columns(pl.col('FormattedFullAddress').str.len_chars().alias('AddressLength'))['AddressLength'].median()\n",
    "\n",
    "print(f'Mean Address Length: {mean_add_len} | Median Address Length: {median_add_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OID_': [72099617],\n",
       " 'FullAddress': ['472 south main street\\n\\ncamp hill, al, 36850'],\n",
       " 'FormattedFullAddress': ['472, south Main Street, Camp Hill, Tallapoosa County, Alabama, 36850'],\n",
       " 'Latitude': ['32.79596540137694'],\n",
       " 'Longitude': ['-85.6535596907001']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "df.select(['OID_', 'FullAddress', 'FormattedFullAddress', 'Latitude', 'Longitude'])[idx].to_dict(as_series=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validating the Formatted Full Address with the Full Address from Nominatim using reverse geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13, Daniels Place, Dixwell, New Haven, Connecticut, 06511, United States\n",
      "41.3177839 -72.932018\n"
     ]
    }
   ],
   "source": [
    "# location = geolocator.geocode(\"13, John Daniels Place, New Haven County, Connecticut, 06511\")\n",
    "# location = geolocator.geocode({\"postalcode\": int(\"06511\"), \"country\": \"US\"})\n",
    "location = geolocator.reverse(['41.317783902424', '-72.9320178229665'])\n",
    "\n",
    "if location is None:\n",
    "    print(\"Location not found.\")\n",
    "else:\n",
    "    print(location.address)\n",
    "    print(location.latitude, location.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4524, Old Caldwell Mill Road, Shelby County, Alabama, 35242\n",
      "33.41236637208968 -86.73952124099591\n"
     ]
    }
   ],
   "source": [
    "idx = 100\n",
    "print(df[idx]['FormattedFullAddress'].item())\n",
    "print(df[idx]['Latitude'].item(), df[idx]['Longitude'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write_parquet(data_dir / 'new_formatted_addresses.parquet', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the newly formatted address matches more closely with the standard open street maps address."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now building the source-target pairs for supervised fine tuning. The source is the unnormalized / address with mistakes and target is the cleaned address. \n",
    "To generate noisy source addresses, we inject the following noise: <br>\n",
    " Noise Types Introduced\n",
    "\n",
    "| Noise Type              | Field           | Description                                                                 |\n",
    "|-------------------------|------------------|-----------------------------------------------------------------------------|\n",
    "| Street Number Removal   | `Add_Number`     | 50% chance to remove the house/building number (`None`)                    |\n",
    "| Character Corruption    | `St_Name`        | 20% per character: replace characters randomly (simulating typos)          |\n",
    "| City Dropping           | `Post_City`      | 30% chance to remove city field                                             |\n",
    "| ZIP Code Truncation     | `Zip_Code`       | 20% chance to truncate ZIP (e.g., `36078` → `3607`)                         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction Fine Tuning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADDRESS_JSON_FORMAT = {\n",
    "    \"AddNum_Pre\": \"\",\n",
    "    \"Add_Number\": \"\",\n",
    "    \"AddNum_Suf\": \"\",\n",
    "    \"St_PreDir\": \"\",\n",
    "    \"St_Name\": \"\",\n",
    "    \"St_PosTyp\": \"\",\n",
    "    \"St_PosDir\": \"\",\n",
    "    \"Building\": \"\",\n",
    "    \"Floor\": \"\",\n",
    "    \"Unit\": \"\",\n",
    "    \"Room\": \"\",\n",
    "    \"Uninc_Comm\": \"\",\n",
    "    \"Inc_Muni\": \"\",\n",
    "    \"County\": \"\",\n",
    "    \"State\": \"\",\n",
    "    \"Zip_Code\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State abbreviation to full name mapping\n",
    "STATE_MAP = {\n",
    "    'TX': 'Texas',\n",
    "    'LA': 'Louisiana',\n",
    "    'ME': 'Maine',\n",
    "    'WY': 'Wyoming',\n",
    "    'KY': 'Kentucky',\n",
    "    'MI': 'Michigan',\n",
    "    'WA': 'Washington',\n",
    "    'VT': 'Vermont',\n",
    "    'ND': 'North Dakota',\n",
    "    'TN': 'Tennessee',\n",
    "    'IN': 'Indiana',\n",
    "    'WV': 'West Virginia',\n",
    "    'MN': 'Minnesota',\n",
    "    'RI': 'Rhode Island',\n",
    "    'DE': 'Delaware',\n",
    "    'IL': 'Illinois',\n",
    "    'SD': 'South Dakota',\n",
    "    'AK': 'Alaska',\n",
    "    'MS': 'Mississippi',\n",
    "    'OK': 'Oklahoma',\n",
    "    'PA': 'Pennsylvania',\n",
    "    'WI': 'Wisconsin',\n",
    "    'NY': 'New York',\n",
    "    'KS': 'Kansas',\n",
    "    'NM': 'New Mexico',\n",
    "    'AZ': 'Arizona',\n",
    "    'SC': 'South Carolina',\n",
    "    'FL': 'Florida',\n",
    "    'NC': 'North Carolina',\n",
    "    'MD': 'Maryland',\n",
    "    'UT': 'Utah',\n",
    "    'NE': 'Nebraska',\n",
    "    'NH': 'New Hampshire',\n",
    "    'VA': 'Virginia',\n",
    "    'GA': 'Georgia',\n",
    "    'AL': 'Alabama',\n",
    "    'CA': 'California',\n",
    "    'MA': 'Massachusetts',\n",
    "    'CT': 'Connecticut',\n",
    "    'AR': 'Arkansas',\n",
    "    'CO': 'Colorado',\n",
    "    'MT': 'Montana',\n",
    "    'DC': 'District of Columbia',\n",
    "    'ID': 'Idaho',\n",
    "    'IA': 'Iowa',\n",
    "    'OH': 'Ohio',\n",
    "    'MO': 'Missouri'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Address Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_address_json(row: dict) -> dict:\n",
    "    \"\"\"Extract address fields from a row to create the address JSON.\"\"\"\n",
    "    result = ADDRESS_JSON_FORMAT.copy()\n",
    "    \n",
    "    field_standardizers = {\n",
    "        \"AddNum_Pre\": lambda x: str(x).strip(),\n",
    "        \"Add_Number\": lambda x: str(x).strip(),\n",
    "        \"AddNum_Suf\": lambda x: str(x).strip(),\n",
    "        \"St_PreDir\": lambda x: str(x).lower().strip(),  # Directionals standardized to lowercase\n",
    "        \"St_Name\": lambda x: str(x).lower().strip(),    # Street names standardized to lowercase\n",
    "        \"St_PosTyp\": lambda x: str(x).lower().strip(),  # Street types standardized to lowercase\n",
    "        \"St_PosDir\": lambda x: str(x).lower().strip(),  # Directionals standardized to lowercase\n",
    "        \"Building\": lambda x: str(x).strip(),\n",
    "        \"Floor\": lambda x: str(x).strip(),\n",
    "        \"Unit\": lambda x: str(x).strip(),\n",
    "        \"Room\": lambda x: str(x).strip(),\n",
    "        \"Uninc_Comm\": lambda x: str(x).lower().strip(), # Community names standardized to lowercase\n",
    "        \"Inc_Muni\": lambda x: str(x).lower().strip(),   # Municipality standardized to lowercase\n",
    "        \"County\": lambda x: str(x).lower().strip(),     # County standardized to lowercase\n",
    "        \"State\": lambda x: str(x).lower().strip(),      # State standardized to lowercase\n",
    "        \"Zip_Code\": lambda x: str(x).strip()\n",
    "    }\n",
    "    \n",
    "    for key in result.keys():\n",
    "        if key in row and row[key] is not None and str(row[key]).strip():\n",
    "            # Apply the appropriate standardization function\n",
    "            if key in field_standardizers:\n",
    "                result[key] = field_standardizers[key](row[key])\n",
    "            else:\n",
    "                result[key] = str(row[key])\n",
    "    \n",
    "    return result\n",
    "\n",
    "def create_address_parsing_task(row: dict) -> tuple:\n",
    "    \"\"\"Create instruction and ground truth for address parsing task.\"\"\"\n",
    "    address_json = extract_address_json(row)\n",
    "    \n",
    "    instruction = \"Parse the following address into a structured JSON with these fields: AddNum_Pre, Add_Number, AddNum_Suf, St_PreDir, St_Name, St_PosTyp, St_PosDir, Building, Floor, Unit, Room, Uninc_Comm, Inc_Muni, County, State, Zip_Code.\"\n",
    "    \n",
    "    address_string = row.get('FormattedFullAddress')\n",
    "\n",
    "    task_instruction = f\"{instruction}\\nAddress: {address_string}\"\n",
    "    ground_truth = json.dumps(address_json, indent=2)\n",
    "    \n",
    "    return task_instruction, ground_truth\n",
    "\n",
    "def build_task1_instructions(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Build an instruction fine-tuning dataset for address parsing (Task 1).\n",
    "    \n",
    "    Args:\n",
    "        df: Input polars DataFrame with address data\n",
    "    \n",
    "    Returns:\n",
    "        A polars DataFrame with instruction fine-tuning tasks\n",
    "    \"\"\"\n",
    "    # Convert polars DataFrame to list of dictionaries for easier processing\n",
    "    rows = df.to_dicts()\n",
    "    \n",
    "    # Create lists to store the results\n",
    "    instructions = []\n",
    "    groundtruths = []\n",
    "    \n",
    "    for row in rows:\n",
    "        # Address Parsing Task\n",
    "        instruction, groundtruth = create_address_parsing_task(row)\n",
    "        \n",
    "        # Append to result lists\n",
    "        instructions.append(instruction)\n",
    "        groundtruths.append(groundtruth)\n",
    "\n",
    "    df = df.with_columns(\n",
    "        pl.Series(name='task1_instruction', values=instructions),\n",
    "        pl.Series(name='task1_groundtruth', values=groundtruths)\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task1 = build_task1_instructions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_instruction(df: pl.DataFrame, idx: int):\n",
    "    record = df[idx]\n",
    "    print(record[\"task1_instruction\"].item())\n",
    "    print(record[\"task1_groundtruth\"].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse the following address into a structured JSON with these fields: AddNum_Pre, Add_Number, AddNum_Suf, St_PreDir, St_Name, St_PosTyp, St_PosDir, Building, Floor, Unit, Room, Uninc_Comm, Inc_Muni, County, State, Zip_Code.\n",
      "Address: 5551, Wares Ferry Road, Montgomery County, Alabama, 36117\n",
      "{\n",
      "  \"AddNum_Pre\": \"\",\n",
      "  \"Add_Number\": \"5551\",\n",
      "  \"AddNum_Suf\": \"\",\n",
      "  \"St_PreDir\": \"\",\n",
      "  \"St_Name\": \"wares ferry\",\n",
      "  \"St_PosTyp\": \"road\",\n",
      "  \"St_PosDir\": \"\",\n",
      "  \"Building\": \"\",\n",
      "  \"Floor\": \"\",\n",
      "  \"Unit\": \"\",\n",
      "  \"Room\": \"\",\n",
      "  \"Uninc_Comm\": \"\",\n",
      "  \"Inc_Muni\": \"nan\",\n",
      "  \"County\": \"montgomery\",\n",
      "  \"State\": \"al\",\n",
      "  \"Zip_Code\": \"36117\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print_instruction(df_task1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Address Entity Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_character_noise(component: str) -> str:\n",
    "#     \"\"\"Add character noise while maintaining string type\"\"\"\n",
    "#     return ''.join([\n",
    "#         random.choice(string.ascii_lowercase) \n",
    "#         if c.isalpha() and random.random() < 0.2 \n",
    "#         else c\n",
    "#         for c in component\n",
    "#     ]) if component else component\n",
    "\n",
    "# def generate_noisy_address(row: Dict, state_map: Dict = STATE_MAP) -> str:\n",
    "#     \"\"\"Generate noisy address with type-safe modifications\"\"\"\n",
    "#     modified = row.copy()\n",
    "    \n",
    "#     # 50% chance to remove street number (set to None)\n",
    "#     if random.random() < 0.5:\n",
    "#         modified['Add_Number'] = None\n",
    "    \n",
    "#     # Add noise to street name (keep as string)\n",
    "#     if modified.get('St_Name'):\n",
    "#         modified['St_Name'] = add_character_noise(str(modified['St_Name']))\n",
    "    \n",
    "#     # 30% chance to remove city (set to None)\n",
    "#     if random.random() < 0.3:\n",
    "#         modified['Post_City'] = None\n",
    "    \n",
    "#     # 20% chance to modify zip code (keep as integer)\n",
    "#     if modified.get('Zip_Code') and random.random() < 0.2:\n",
    "#         zip_code = int(modified['Zip_Code'])\n",
    "#         if 10000 <= zip_code <= 99999:\n",
    "#             modified['Zip_Code'] = zip_code // 10  # Truncate last digit\n",
    "    \n",
    "#     return format_usdot_to_freeform_granular(modified, state_map)\n",
    "\n",
    "# def create_address_pairs(df: pl.DataFrame, n_noisy_varient_per_add: int = 3) -> pl.DataFrame:\n",
    "#     \"\"\"Generate address pairs with schema consistency\"\"\"\n",
    "#     results = []\n",
    "\n",
    "#     for row in df.to_dicts():\n",
    "#         oid = row['OID_']\n",
    "#         state = row['State']\n",
    "\n",
    "#         # Original clean target\n",
    "#         clean_target = row['FormattedFullAddress']\n",
    "        \n",
    "#         # Add clean pair\n",
    "#         results.append({\n",
    "#             'oid': oid,\n",
    "#             'source': clean_target,\n",
    "#             'target': clean_target,\n",
    "#             'state': state\n",
    "#         })\n",
    "        \n",
    "#         # Generate n noisy variants\n",
    "#         for _ in range(n_noisy_varient_per_add):\n",
    "#             noisy_source = generate_noisy_address(row)\n",
    "#             results.append({\n",
    "#                 'oid': oid,\n",
    "#                 'source': noisy_source,\n",
    "#                 'target': clean_target,\n",
    "#                 'state': state\n",
    "#             })\n",
    "#     # Ensure schema consistency\n",
    "#     return pl.DataFrame(results).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noisy_df = create_address_pairs(df_task1, n_noisy_varient_per_add=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_task1.join(noisy_df, left_on=['OID_'], right_on=['oid', ], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_noise_into_json(cleaned_json_str: str, noise_level: str = \"medium\") -> str:\n",
    "    \"\"\"\n",
    "    Inject realistic noise into a well-formatted Address JSON string.\n",
    "    Combines structured field-specific transformations with random noise.\n",
    "    \n",
    "    Args:\n",
    "        cleaned_json_str: A string containing well-formatted Address JSON\n",
    "        noise_level: Level of noise to inject (\"low\", \"medium\", \"high\", \"extreme\")\n",
    "        \n",
    "    Returns:\n",
    "        A string containing the corrupted Address JSON\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse the JSON string\n",
    "        if isinstance(cleaned_json_str, str):\n",
    "            cleaned_json = json.loads(cleaned_json_str)\n",
    "        else:\n",
    "            # If it's already a dict, use it directly\n",
    "            cleaned_json = cleaned_json_str\n",
    "            \n",
    "        # Make a copy to modify\n",
    "        noisy_json = cleaned_json.copy()\n",
    "        \n",
    "        # Adjust noise parameters based on noise level\n",
    "        if noise_level == \"low\":\n",
    "            error_rate = 0.15\n",
    "            max_fields_to_modify = 2\n",
    "            field_weights = {'typo': 0.4, 'empty': 0.1, 'alternate_form': 0.3, 'random_noise': 0.2}\n",
    "            empty_field_probability = 0.1\n",
    "            special_mod_probability = 0.05\n",
    "        elif noise_level == \"medium\":\n",
    "            error_rate = 0.25\n",
    "            max_fields_to_modify = 4\n",
    "            field_weights = {'typo': 0.3, 'empty': 0.2, 'alternate_form': 0.2, 'random_noise': 0.3}\n",
    "            empty_field_probability = 0.2\n",
    "            special_mod_probability = 0.15\n",
    "        elif noise_level == \"high\":\n",
    "            error_rate = 0.4\n",
    "            max_fields_to_modify = 6\n",
    "            field_weights = {'typo': 0.25, 'empty': 0.25, 'alternate_form': 0.15, 'random_noise': 0.35}\n",
    "            empty_field_probability = 0.3\n",
    "            special_mod_probability = 0.25\n",
    "        else:  # extreme\n",
    "            error_rate = 0.6\n",
    "            max_fields_to_modify = 8\n",
    "            field_weights = {'typo': 0.2, 'empty': 0.3, 'alternate_form': 0.1, 'random_noise': 0.4}\n",
    "            empty_field_probability = 0.4\n",
    "            special_mod_probability = 0.4\n",
    "        \n",
    "        # Helper functions for adding different types of noise\n",
    "        def add_typo(text: str) -> str:\n",
    "            \"\"\"Add realistic typos to text strings with higher error rate\"\"\"\n",
    "            if not text or len(text) < 2:\n",
    "                return text\n",
    "                \n",
    "            result = list(text)\n",
    "            \n",
    "            # Multiple errors possible in longer text\n",
    "            num_errors = 1\n",
    "            if len(text) > 5:\n",
    "                num_errors = random.randint(1, min(len(text) // 3, 3))\n",
    "            \n",
    "            for _ in range(num_errors):\n",
    "                # Determine error type based on text length\n",
    "                error_types = []\n",
    "                \n",
    "                if len(text) >= 2:\n",
    "                    error_types.extend(['swap', 'delete', 'substitute', 'insert_random'])\n",
    "                if len(text) >= 4:\n",
    "                    error_types.extend(['case', 'multiple_insert'])\n",
    "                if len(text) >= 6:\n",
    "                    error_types.extend(['double_letter', 'remove_multiple'])\n",
    "                    \n",
    "                error_type = random.choice(error_types)\n",
    "                \n",
    "                if error_type == 'swap' and len(text) >= 2:\n",
    "                    # Swap two adjacent characters\n",
    "                    pos = random.randint(0, len(result) - 2)\n",
    "                    if pos < len(result) - 1:  # Ensure we're not at the end\n",
    "                        result[pos], result[pos + 1] = result[pos + 1], result[pos]\n",
    "                    \n",
    "                elif error_type == 'delete':\n",
    "                    # Delete a character\n",
    "                    if len(result) > 1:  # Ensure we don't delete all characters\n",
    "                        pos = random.randint(0, len(result) - 1)\n",
    "                        result[pos] = ''\n",
    "                    \n",
    "                elif error_type == 'substitute':\n",
    "                    # Substitute a character with a similar one\n",
    "                    adjacent_keys = {\n",
    "                        'a': 'sqzw', 'b': 'vghn', 'c': 'xdfv', 'd': 'serfcx', 'e': 'wrsdf',\n",
    "                        'f': 'drtgv', 'g': 'ftyhv', 'h': 'gyujbn', 'i': 'ujko', 'j': 'huiknm',\n",
    "                        'k': 'jiolm', 'l': 'kop', 'm': 'njk', 'n': 'bhjm', 'o': 'iklp',\n",
    "                        'p': 'ol', 'q': 'wa', 'r': 'edft', 's': 'awedxz', 't': 'rfgy',\n",
    "                        'u': 'yhji', 'v': 'cfgb', 'w': 'qase', 'x': 'zsdc', 'y': 'tghu',\n",
    "                        'z': 'asx', '0': '9po', '1': '2q', '2': '1w3', '3': '2e4', '4': '3r5',\n",
    "                        '5': '4t6', '6': '5y7', '7': '6u8', '8': '7i9', '9': '8o0'\n",
    "                    }\n",
    "                    \n",
    "                    if result:  # Ensure we have characters to work with\n",
    "                        pos = random.randint(0, len(result) - 1)\n",
    "                        if pos < len(result):  # Safety check\n",
    "                            char = result[pos].lower()\n",
    "                            if char in adjacent_keys:\n",
    "                                replacement = random.choice(adjacent_keys[char])\n",
    "                                # Match case if original was uppercase\n",
    "                                if result[pos].isupper():\n",
    "                                    replacement = replacement.upper()\n",
    "                                result[pos] = replacement\n",
    "                        \n",
    "                elif error_type == 'case':\n",
    "                    # Change case of one or more characters\n",
    "                    num_to_change = random.randint(1, min(len(result), 3))\n",
    "                    for _ in range(num_to_change):\n",
    "                        if result:  # Ensure we have characters\n",
    "                            pos = random.randint(0, len(result) - 1)\n",
    "                            if pos < len(result) and result[pos].isalpha():  # Safety check\n",
    "                                if result[pos].islower():\n",
    "                                    result[pos] = result[pos].upper()\n",
    "                                else:\n",
    "                                    result[pos] = result[pos].lower()\n",
    "                        \n",
    "                elif error_type == 'insert_random':\n",
    "                    # Insert random characters\n",
    "                    if len(result) > 0:  # Ensure we have a string to work with\n",
    "                        pos = random.randint(0, len(result))\n",
    "                        random_char = random.choice(string.ascii_lowercase + string.digits)\n",
    "                        if pos <= len(result):  # Safety check\n",
    "                            result.insert(pos, random_char)\n",
    "                \n",
    "                elif error_type == 'multiple_insert':\n",
    "                    # Insert multiple random characters\n",
    "                    num_inserts = random.randint(1, 3)\n",
    "                    for _ in range(num_inserts):\n",
    "                        if len(result) > 0:  # Safety check\n",
    "                            pos = random.randint(0, len(result))\n",
    "                            random_char = random.choice(string.ascii_lowercase + string.digits)\n",
    "                            if pos <= len(result):  # Safety check\n",
    "                                result.insert(pos, random_char)\n",
    "                \n",
    "                elif error_type == 'double_letter':\n",
    "                    # Double a letter (common typo)\n",
    "                    if result:  # Ensure we have characters\n",
    "                        pos = random.randint(0, len(result) - 1)\n",
    "                        if pos < len(result) and result[pos].isalpha():  # Safety check\n",
    "                            result.insert(pos + 1, result[pos])\n",
    "                \n",
    "                elif error_type == 'remove_multiple':\n",
    "                    # Remove multiple characters in a row\n",
    "                    if len(result) > 3:  # Ensure we have enough characters\n",
    "                        start_pos = random.randint(0, len(result) - 3)\n",
    "                        num_to_remove = random.randint(1, min(len(result) - start_pos - 1, 3))\n",
    "                        for i in range(num_to_remove):\n",
    "                            if start_pos < len(result):  # Safety check\n",
    "                                result[start_pos] = ''\n",
    "                                # Don't increment start_pos since we're removing items in place\n",
    "            \n",
    "            return ''.join(result)\n",
    "            \n",
    "        def add_character_noise(text: str) -> str:\n",
    "            \"\"\"Add character noise while maintaining string type (with higher error rate)\"\"\"\n",
    "            if not text:\n",
    "                return text\n",
    "                \n",
    "            result = []\n",
    "            for c in text:\n",
    "                # Higher chance of inserting random characters\n",
    "                if c.isalpha() and random.random() < error_rate:\n",
    "                    result.append(random.choice(string.ascii_lowercase + string.digits))\n",
    "                else:\n",
    "                    result.append(c)\n",
    "                    \n",
    "                # Chance to insert an extra character after\n",
    "                if random.random() < error_rate / 2:\n",
    "                    result.append(random.choice(string.ascii_lowercase + string.digits))\n",
    "                    \n",
    "            # Chance to remove random characters (up to 20%)\n",
    "            if len(result) > 5 and random.random() < error_rate:\n",
    "                num_to_remove = random.randint(1, max(1, int(len(result) * 0.2)))\n",
    "                indices_to_remove = random.sample(range(len(result)), min(num_to_remove, len(result)))\n",
    "                for i in sorted(indices_to_remove, reverse=True):\n",
    "                    if 0 <= i < len(result):  # Safety check\n",
    "                        result.pop(i)\n",
    "                        \n",
    "            return ''.join(result)\n",
    "        \n",
    "        def field_specific_noise(field: str, value: str) -> str:\n",
    "            \"\"\"Apply field-specific transformations and abbreviations\"\"\"\n",
    "            if not value:\n",
    "                return value\n",
    "                \n",
    "            # Sometimes use original character noise approach with higher probability\n",
    "            if random.random() < error_rate:\n",
    "                return add_character_noise(value)\n",
    "                \n",
    "            # Field-specific common errors/variations\n",
    "            if field == \"St_PosTyp\":\n",
    "                # Street type variations\n",
    "                mapping = {\n",
    "                    'street': ['st', 'str', 'strt', 'stret', 'streeet', 'strret'],\n",
    "                    'avenue': ['ave', 'av', 'aven', 'avenu', 'avnue', 'avne'],\n",
    "                    'boulevard': ['blvd', 'boul', 'blv', 'bld', 'boulevrd', 'boulevd'],\n",
    "                    'road': ['rd', 'rod', 'rad', 'rroad', 'roadd'],\n",
    "                    'lane': ['ln', 'la', 'lne', 'lanne'],\n",
    "                    'drive': ['dr', 'drv', 'dirve', 'driv', 'drvie'],\n",
    "                    'circle': ['cir', 'circ', 'crcl', 'cicle'],\n",
    "                    'court': ['ct', 'crt', 'cort', 'courrt'],\n",
    "                    'place': ['pl', 'plc', 'plce', 'plac']\n",
    "                }\n",
    "                \n",
    "                value_lower = value.lower()\n",
    "                for full, variants in mapping.items():\n",
    "                    if value_lower == full:\n",
    "                        return random.choice(variants)\n",
    "                    elif value_lower in variants:\n",
    "                        # Randomly expand abbreviation\n",
    "                        if random.random() < 0.4:\n",
    "                            return full\n",
    "                        else:\n",
    "                            return value\n",
    "            \n",
    "            elif field in [\"St_PreDir\", \"St_PosDir\"]:\n",
    "                # Directional variations\n",
    "                mapping = {\n",
    "                    'north': ['n', 'no', 'nrth', 'nth', 'nort', 'norht'],\n",
    "                    'south': ['s', 'so', 'sth', 'sout', 'souht', 'souh'],\n",
    "                    'east': ['e', 'ea', 'est', 'eas', 'esat', 'esst'],\n",
    "                    'west': ['w', 'wt', 'wst', 'wes', 'vest', 'wesst'],\n",
    "                    'northwest': ['nw', 'n w', 'nortwest', 'north west', 'nor west', 'northwst'],\n",
    "                    'northeast': ['ne', 'n e', 'northest', 'north east', 'nrth east', 'noreast'],\n",
    "                    'southwest': ['sw', 's w', 'soutwest', 'south west', 'sout west', 'southwst'],\n",
    "                    'southeast': ['se', 's e', 'southest', 'south east', 'sout east', 'sutheast']\n",
    "                }\n",
    "                \n",
    "                value_lower = value.lower()\n",
    "                for full, variants in mapping.items():\n",
    "                    if value_lower == full:\n",
    "                        return random.choice(variants)\n",
    "                    elif value_lower in variants:\n",
    "                        # Randomly expand abbreviation\n",
    "                        if random.random() < 0.4:\n",
    "                            return full\n",
    "                        else:\n",
    "                            return value\n",
    "            \n",
    "            elif field == \"State\":\n",
    "                # State abbreviation/full name variations with typos\n",
    "                state_mapping = {\n",
    "                    'al': ['alabama', 'alabamma', 'alambama', 'alabma'],\n",
    "                    'ak': ['alaska', 'alasca', 'alaskaa', 'alaka'],\n",
    "                    'az': ['arizona', 'arizonna', 'arizonaa', 'arizon'],\n",
    "                    'ar': ['arkansas', 'arkansass', 'arkansa', 'arkanss'],\n",
    "                    'ca': ['california', 'califrnia', 'califronia', 'californa'],\n",
    "                    'co': ['colorado', 'colordo', 'colorada', 'colorodo'],\n",
    "                    'ct': ['connecticut', 'conneticut', 'conecticut', 'conneticut'],\n",
    "                    'de': ['delaware', 'deleware', 'delware', 'delawar'],\n",
    "                    'fl': ['florida', 'flrida', 'flordia', 'floria'],\n",
    "                    'ga': ['georgia', 'gergia', 'geogia', 'georga'],\n",
    "                    'hi': ['hawaii', 'hawai', 'hawii', 'hawai'],\n",
    "                    'id': ['idaho', 'idao', 'idahoo', 'ideho'],\n",
    "                    'il': ['illinois', 'illnois', 'illinoi', 'illinos'],\n",
    "                    'in': ['indiana', 'indana', 'indianna', 'indina'],\n",
    "                    'ia': ['iowa', 'ioa', 'iowaa', 'iwa'],\n",
    "                    'ks': ['kansas', 'knsas', 'kansa', 'kanss'],\n",
    "                    'ky': ['kentucky', 'kentuky', 'kentuckyy', 'kentuckey'],\n",
    "                    'la': ['louisiana', 'lousiana', 'louisian', 'louisana'],\n",
    "                    'me': ['maine', 'main', 'manie', 'mane'],\n",
    "                    'md': ['maryland', 'mayland', 'marland', 'mariland'],\n",
    "                    'ma': ['massachusetts', 'massachusets', 'massachsetts', 'massachusets'],\n",
    "                    'mi': ['michigan', 'michign', 'michgan', 'michigann'],\n",
    "                    'mn': ['minnesota', 'minesota', 'minnesot', 'minnisota'],\n",
    "                    'ms': ['mississippi', 'missisippi', 'mississipi', 'missisipi'],\n",
    "                    'mo': ['missouri', 'misouri', 'missori', 'missoury'],\n",
    "                    'mt': ['montana', 'montna', 'montanna', 'montaa'],\n",
    "                    'ne': ['nebraska', 'nebraka', 'nebaska', 'nebraksa'],\n",
    "                    'nv': ['nevada', 'nevda', 'nevaa', 'neveda'],\n",
    "                    'nh': ['new hampshire', 'new hamshire', 'new hampshir', 'new hampsher'],\n",
    "                    'nj': ['new jersey', 'new jersery', 'new jrsey', 'new jersy'],\n",
    "                    'nm': ['new mexico', 'new mexio', 'new mexcio', 'new mexco'],\n",
    "                    'ny': ['new york', 'new yrk', 'new yorke', 'newyork'],\n",
    "                    'nc': ['north carolina', 'north carolia', 'north carlina', 'north carolna'],\n",
    "                    'nd': ['north dakota', 'north dakta', 'north dakoa', 'north dakotta'],\n",
    "                    'oh': ['ohio', 'ohi', 'ohioo', 'ohio'],\n",
    "                    'ok': ['oklahoma', 'oklahma', 'oklahoa', 'oklahome'],\n",
    "                    'or': ['oregon', 'orgon', 'oregan', 'oreegon'],\n",
    "                    'pa': ['pennsylvania', 'pennsylvnia', 'pensylvania', 'pennsylvana'],\n",
    "                    'ri': ['rhode island', 'rhod island', 'rhode islnd', 'rode island'],\n",
    "                    'sc': ['south carolina', 'south carolia', 'south carlina', 'south carolna'],\n",
    "                    'sd': ['south dakota', 'south dakta', 'south dakoa', 'south dakotta'],\n",
    "                    'tn': ['tennessee', 'tennesee', 'tennese', 'tennessse'],\n",
    "                    'tx': ['texas', 'texass', 'texs', 'texxas'],\n",
    "                    'ut': ['utah', 'utahh', 'uta', 'utha'],\n",
    "                    'vt': ['vermont', 'vermnt', 'vermon', 'vermot'],\n",
    "                    'va': ['virginia', 'virgina', 'virgnia', 'virginiaa'],\n",
    "                    'wa': ['washington', 'washingtn', 'washinton', 'washingtton'],\n",
    "                    'wv': ['west virginia', 'west virgina', 'west virgnia', 'west virginiaa'],\n",
    "                    'wi': ['wisconsin', 'wisconsn', 'wisconsinn', 'wiscosin'],\n",
    "                    'wy': ['wyoming', 'wyomng', 'wyomin', 'woming']\n",
    "                }\n",
    "                \n",
    "                value_lower = value.lower()\n",
    "                \n",
    "                # If it's an abbreviation, possibly expand it (with typos)\n",
    "                if value_lower in state_mapping:\n",
    "                    if random.random() < 0.6:  # Higher chance of expansion\n",
    "                        full_options = state_mapping[value_lower]\n",
    "                        return random.choice(full_options)\n",
    "                \n",
    "                # If it's a full name, possibly abbreviate it or replace with another typo version\n",
    "                for abbr, full_options in state_mapping.items():\n",
    "                    if value_lower in full_options:\n",
    "                        if random.random() < 0.4:\n",
    "                            return abbr\n",
    "                        else:\n",
    "                            # Use a different typo variation\n",
    "                            other_options = [opt for opt in full_options if opt != value_lower]\n",
    "                            if other_options:\n",
    "                                return random.choice(other_options)\n",
    "            \n",
    "            # Default: apply standard typo with higher error rate\n",
    "            return add_typo(value)\n",
    "        \n",
    "        # Add special modifications based on field type with higher chance of changes\n",
    "        def field_specific_modifications():\n",
    "            # Handle Add_Number specially - might remove completely or modify\n",
    "            if \"Add_Number\" in noisy_json and noisy_json[\"Add_Number\"]:\n",
    "                mod_choice = random.choices(\n",
    "                    ['remove', 'truncate', 'add_digit', 'transpose'], \n",
    "                    weights=[0.3, 0.2, 0.2, 0.3], \n",
    "                    k=1\n",
    "                )[0]\n",
    "                \n",
    "                if mod_choice == 'remove':\n",
    "                    noisy_json[\"Add_Number\"] = \"\"\n",
    "                elif mod_choice == 'truncate' and len(str(noisy_json[\"Add_Number\"])) > 1:\n",
    "                    # Remove first or last digit\n",
    "                    if random.random() < 0.5:\n",
    "                        noisy_json[\"Add_Number\"] = str(noisy_json[\"Add_Number\"])[1:]\n",
    "                    else:\n",
    "                        noisy_json[\"Add_Number\"] = str(noisy_json[\"Add_Number\"])[:-1]\n",
    "                elif mod_choice == 'add_digit':\n",
    "                    # Add a random digit at beginning or end\n",
    "                    digit = str(random.randint(0, 9))\n",
    "                    if random.random() < 0.5:\n",
    "                        noisy_json[\"Add_Number\"] = digit + str(noisy_json[\"Add_Number\"])\n",
    "                    else:\n",
    "                        noisy_json[\"Add_Number\"] = str(noisy_json[\"Add_Number\"]) + digit\n",
    "                elif mod_choice == 'transpose' and len(str(noisy_json[\"Add_Number\"])) > 1:\n",
    "                    # Transpose digits\n",
    "                    num_str = str(noisy_json[\"Add_Number\"])\n",
    "                    pos = random.randint(0, len(num_str) - 2)\n",
    "                    noisy_json[\"Add_Number\"] = num_str[:pos] + num_str[pos+1] + num_str[pos] + num_str[pos+2:]\n",
    "                \n",
    "            # Handle Zip_Code specially - might remove, truncate, or modify\n",
    "            if \"Zip_Code\" in noisy_json and noisy_json[\"Zip_Code\"]:\n",
    "                mod_choice = random.choices(\n",
    "                    ['remove', 'truncate', 'add_digit', 'transpose'], \n",
    "                    weights=[0.2, 0.3, 0.2, 0.3], \n",
    "                    k=1\n",
    "                )[0]\n",
    "                \n",
    "                if mod_choice == 'remove':\n",
    "                    noisy_json[\"Zip_Code\"] = \"\"\n",
    "                elif mod_choice == 'truncate' and len(str(noisy_json[\"Zip_Code\"])) > 1:\n",
    "                    # Remove last digit(s)\n",
    "                    num_to_remove = random.randint(1, min(2, len(str(noisy_json[\"Zip_Code\"])) - 1))\n",
    "                    noisy_json[\"Zip_Code\"] = str(noisy_json[\"Zip_Code\"])[:-num_to_remove]\n",
    "                elif mod_choice == 'add_digit':\n",
    "                    # Add a random digit at beginning or end\n",
    "                    digit = str(random.randint(0, 9))\n",
    "                    if random.random() < 0.3:  # less likely at beginning\n",
    "                        noisy_json[\"Zip_Code\"] = digit + str(noisy_json[\"Zip_Code\"])\n",
    "                    else:\n",
    "                        noisy_json[\"Zip_Code\"] = str(noisy_json[\"Zip_Code\"]) + digit\n",
    "                elif mod_choice == 'transpose' and len(str(noisy_json[\"Zip_Code\"])) > 1:\n",
    "                    # Transpose digits\n",
    "                    num_str = str(noisy_json[\"Zip_Code\"])\n",
    "                    pos = random.randint(0, len(num_str) - 2)\n",
    "                    noisy_json[\"Zip_Code\"] = num_str[:pos] + num_str[pos+1] + num_str[pos] + num_str[pos+2:]\n",
    "            \n",
    "            # Possible random structural changes with higher probability\n",
    "            if random.random() < special_mod_probability:\n",
    "                struct_mod = random.choice([\n",
    "                    'merge_fields', 'swap_fields', 'duplicate_content', 'add_suffix', 'add_prefix'\n",
    "                ])\n",
    "                \n",
    "                if struct_mod == 'merge_fields':\n",
    "                    # Merge content of two fields\n",
    "                    field_pairs = [\n",
    "                        (\"St_Name\", \"St_PosTyp\"),\n",
    "                        (\"AddNum_Pre\", \"Add_Number\"),\n",
    "                        (\"Add_Number\", \"St_Name\"),\n",
    "                        (\"St_PreDir\", \"St_Name\"),\n",
    "                        (\"Inc_Muni\", \"County\")\n",
    "                    ]\n",
    "                    valid_pairs = [(f1, f2) for f1, f2 in field_pairs \n",
    "                                  if noisy_json.get(f1) and noisy_json.get(f2)]\n",
    "                    \n",
    "                    if valid_pairs:\n",
    "                        f1, f2 = random.choice(valid_pairs)\n",
    "                        # Merge with or without space\n",
    "                        separator = \"\" if random.random() < 0.5 else \" \"\n",
    "                        noisy_json[f1] = str(noisy_json[f1]) + separator + str(noisy_json[f2])\n",
    "                        noisy_json[f2] = \"\"\n",
    "                \n",
    "                elif struct_mod == 'swap_fields':\n",
    "                    # Swap content of two fields\n",
    "                    field_pairs = [\n",
    "                        (\"St_Name\", \"Inc_Muni\"),\n",
    "                        (\"County\", \"Inc_Muni\"),\n",
    "                        (\"St_PreDir\", \"St_PosDir\"),\n",
    "                        (\"AddNum_Pre\", \"AddNum_Suf\")\n",
    "                    ]\n",
    "                    valid_pairs = [(f1, f2) for f1, f2 in field_pairs \n",
    "                                  if noisy_json.get(f1) and noisy_json.get(f2)]\n",
    "                    \n",
    "                    if valid_pairs:\n",
    "                        f1, f2 = random.choice(valid_pairs)\n",
    "                        noisy_json[f1], noisy_json[f2] = noisy_json[f2], noisy_json[f1]\n",
    "                \n",
    "                elif struct_mod == 'duplicate_content':\n",
    "                    # Duplicate content from one field to another\n",
    "                    source_fields = [f for f in noisy_json.keys() if noisy_json.get(f)]\n",
    "                    target_fields = [f for f in noisy_json.keys() if not noisy_json.get(f)]\n",
    "                    \n",
    "                    if source_fields and target_fields:\n",
    "                        source = random.choice(source_fields)\n",
    "                        target = random.choice(target_fields)\n",
    "                        noisy_json[target] = noisy_json[source]\n",
    "                \n",
    "                elif struct_mod == 'add_suffix' and \"St_Name\" in noisy_json and noisy_json[\"St_Name\"]:\n",
    "                    # Add random suffix to street name\n",
    "                    suffixes = [\"st\", \"street\", \"ave\", \"avenue\", \"rd\", \"road\"]\n",
    "                    noisy_json[\"St_Name\"] += \" \" + random.choice(suffixes)\n",
    "                \n",
    "                elif struct_mod == 'add_prefix' and \"St_Name\" in noisy_json and noisy_json[\"St_Name\"]:\n",
    "                    # Add random prefix to street name\n",
    "                    prefixes = [\"n\", \"s\", \"e\", \"w\", \"north\", \"south\", \"east\", \"west\"]\n",
    "                    noisy_json[\"St_Name\"] = random.choice(prefixes) + \" \" + noisy_json[\"St_Name\"]\n",
    "            \n",
    "            # Random deletion of additional fields\n",
    "            num_extra_fields_to_delete = random.randint(0, 2)\n",
    "            potential_fields_to_delete = [\"Inc_Muni\", \"County\", \"Room\", \"Unit\", \"Building\", \"Floor\", \"St_PosDir\"]\n",
    "            fields_with_values = [f for f in potential_fields_to_delete if f in noisy_json and noisy_json[f]]\n",
    "            \n",
    "            if fields_with_values:\n",
    "                fields_to_delete = random.sample(\n",
    "                    fields_with_values, \n",
    "                    min(num_extra_fields_to_delete, len(fields_with_values))\n",
    "                )\n",
    "                for field in fields_to_delete:\n",
    "                    noisy_json[field] = \"\"\n",
    "        \n",
    "        # Select random fields to modify (with more fields based on noise level)\n",
    "        all_fields = list(noisy_json.keys())\n",
    "        non_empty_fields = [field for field in all_fields if noisy_json[field]]\n",
    "        \n",
    "        # Only corrupt fields that have values\n",
    "        if non_empty_fields:\n",
    "            # Determine how many fields to modify (based on noise level)\n",
    "            num_fields_to_modify = random.randint(1, min(max_fields_to_modify, len(non_empty_fields)))\n",
    "            fields_to_modify = random.sample(non_empty_fields, num_fields_to_modify)\n",
    "            \n",
    "            for field in fields_to_modify:\n",
    "                field_value = noisy_json[field]\n",
    "                if not field_value:\n",
    "                    continue\n",
    "                    \n",
    "                # Choose modification type with weights based on noise level\n",
    "                mod_type = random.choices(\n",
    "                    ['typo', 'empty', 'alternate_form', 'random_noise'], \n",
    "                    weights=[\n",
    "                        field_weights['typo'], \n",
    "                        field_weights['empty'], \n",
    "                        field_weights['alternate_form'], \n",
    "                        field_weights['random_noise']\n",
    "                    ], \n",
    "                    k=1\n",
    "                )[0]\n",
    "                \n",
    "                if mod_type == 'typo':\n",
    "                    # Add a typo to the field\n",
    "                    noisy_json[field] = field_specific_noise(field, field_value)\n",
    "                elif mod_type == 'empty':\n",
    "                    # Empty the field\n",
    "                    noisy_json[field] = \"\"\n",
    "                elif mod_type == 'alternate_form':\n",
    "                    # Apply field-specific transformation\n",
    "                    noisy_json[field] = field_specific_noise(field, field_value)\n",
    "                elif mod_type == 'random_noise':\n",
    "                    # Apply original random character insertion approach\n",
    "                    noisy_json[field] = add_character_noise(field_value)\n",
    "        \n",
    "        # Apply additional field-specific modifications\n",
    "        field_specific_modifications()\n",
    "        \n",
    "        # Return as formatted JSON string\n",
    "        return json.dumps(noisy_json, indent=2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # If any error occurs, return the original string\n",
    "        print(f\"Error injecting noise: {str(e)}\")\n",
    "        return cleaned_json_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_task2_instruction_dataset(df, n_noise_variants_per_address: int = 3):\n",
    "    \"\"\"\n",
    "    Create Task 2 (entity correction) instruction dataset with multiple noise variants.\n",
    "    \n",
    "    Args:\n",
    "        df: A DataFrame containing task1_groundtruth column with clean Address JSON\n",
    "        n_noise_variants_per_address: Number of noisy variants to create per clean address\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with expanded rows containing task2_instruction and task2_groundtruth columns\n",
    "    \"\"\"\n",
    "    # Convert the input DataFrame to a list of dictionaries for easier manipulation\n",
    "    rows_as_dicts = df.to_dicts()\n",
    "    all_rows = []\n",
    "    \n",
    "    for row_idx, original_row_dict in enumerate(rows_as_dicts):\n",
    "        # Get the clean JSON from Task 1\n",
    "        clean_json = original_row_dict['task1_groundtruth']\n",
    "        \n",
    "        # Create multiple noisy variants\n",
    "        for variant_idx in range(n_noise_variants_per_address):\n",
    "            # Create a copy of the original row\n",
    "            new_row = original_row_dict.copy()\n",
    "            \n",
    "            # Determine noise level - mix of different noise levels\n",
    "            if variant_idx == 0:\n",
    "                noise_level = \"medium\"  # First variant is medium noise\n",
    "            elif variant_idx == n_noise_variants_per_address - 1:\n",
    "                noise_level = \"extreme\"  # Last variant is extreme noise\n",
    "            else:\n",
    "                # Other variants are random levels\n",
    "                noise_level = random.choice([\"low\", \"medium\", \"high\", \"extreme\"])\n",
    "            \n",
    "            # Inject noise to create corrupted JSON\n",
    "            noisy_json = inject_noise_into_json(clean_json, noise_level=noise_level)\n",
    "            \n",
    "            # Create the instruction\n",
    "            task2_instruction = f\"Fix the formatting, structure, correct any existing entities, or predict/add new values to the appropriate entities of this Address JSON. Expand common abbreviations (like st→street, ave→avenue), correct obvious errors (like leading zeros in numbers), generate new values to the appropriate entities, and standardize capitalization. Keep empty fields as empty strings. Do not return anything other than corrected Address JSON\\nAddress JSON: {noisy_json}\"\n",
    "            \n",
    "            # The ground truth is the original clean JSON\n",
    "            task2_groundtruth = clean_json\n",
    "            \n",
    "            # Add task2 columns\n",
    "            new_row['task2_instruction'] = task2_instruction\n",
    "            new_row['task2_groundtruth'] = task2_groundtruth\n",
    "            new_row['noise_level'] = noise_level\n",
    "            new_row['variant_idx'] = variant_idx\n",
    "            \n",
    "            # Add to results\n",
    "            all_rows.append(new_row)\n",
    "    \n",
    "    # Ensure consistent data types by explicitly defining the schema\n",
    "    # First get a list of all columns in the new rows\n",
    "    column_names = list(all_rows[0].keys())\n",
    "    \n",
    "    # Create a schema to help with data type consistency\n",
    "    schema = {}\n",
    "    for col in column_names:\n",
    "        # Sample the first value to determine type\n",
    "        sample_val = all_rows[0][col]\n",
    "        \n",
    "        if isinstance(sample_val, int):\n",
    "            schema[col] = pl.Int64\n",
    "        elif isinstance(sample_val, float):\n",
    "            schema[col] = pl.Float64\n",
    "        else:\n",
    "            schema[col] = pl.Utf8  # Default to string for everything else\n",
    "    \n",
    "    result_df = pl.DataFrame(all_rows, schema=schema)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 1404780\n"
     ]
    }
   ],
   "source": [
    "df_task2 = create_task2_instruction_dataset(df_task1, n_noise_variants_per_address=3)\n",
    "df_task2 = df_task2.unique()\n",
    "print(f'Number of samples: {len(df_task2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Address: 2807, New Hope Road, Marianna, Jackson County, Florida, 32448\n",
      "Input:\n",
      "Fix the formatting, structure, correct any existing entities, or predict/add new values to the appropriate entities of this Address JSON. Expand common abbreviations (like st→street, ave→avenue), correct obvious errors (like leading zeros in numbers), generate new values to the appropriate entities, and standardize capitalization. Keep empty fields as empty strings. Do not return anything other than corrected Address JSON\n",
      "Address JSON: {\n",
      "  \"AddNum_Pre\": \"\",\n",
      "  \"Add_Number\": \"2870\",\n",
      "  \"AddNum_Suf\": \"\",\n",
      "  \"St_PreDir\": \"\",\n",
      "  \"St_Name\": \"nn8 hofp3e\",\n",
      "  \"St_PosTyp\": \"\",\n",
      "  \"St_PosDir\": \"\",\n",
      "  \"Building\": \"\",\n",
      "  \"Floor\": \"\",\n",
      "  \"Unit\": \"\",\n",
      "  \"Room\": \"\",\n",
      "  \"Uninc_Comm\": \"mar4nnh\",\n",
      "  \"Inc_Muni\": \"\",\n",
      "  \"County\": \"\",\n",
      "  \"State\": \"fl\",\n",
      "  \"Zip_Code\": \"324\"\n",
      "}\n",
      "Target:\n",
      "{\n",
      "  \"AddNum_Pre\": \"\",\n",
      "  \"Add_Number\": \"2807\",\n",
      "  \"AddNum_Suf\": \"\",\n",
      "  \"St_PreDir\": \"\",\n",
      "  \"St_Name\": \"new hope\",\n",
      "  \"St_PosTyp\": \"road\",\n",
      "  \"St_PosDir\": \"\",\n",
      "  \"Building\": \"\",\n",
      "  \"Floor\": \"\",\n",
      "  \"Unit\": \"\",\n",
      "  \"Room\": \"\",\n",
      "  \"Uninc_Comm\": \"marianna\",\n",
      "  \"Inc_Muni\": \"nan\",\n",
      "  \"County\": \"jackson\",\n",
      "  \"State\": \"fl\",\n",
      "  \"Zip_Code\": \"32448\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "record = df_task2[3000]\n",
    "print(f\"Correct Address: {record['FormattedFullAddress'].item()}\")\n",
    "print(f\"Input:\\n{record['task2_instruction'].item()}\")\n",
    "print(f\"Target:\\n{record['task2_groundtruth'].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Schema([('OID_', Int64),\n",
       "        ('FormattedFullAddress', String),\n",
       "        ('task1_instruction', String),\n",
       "        ('task1_groundtruth', String),\n",
       "        ('task2_instruction', String),\n",
       "        ('task2_groundtruth', String)])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_task2.select(['OID_', 'FormattedFullAddress', 'task1_instruction', 'task1_groundtruth', 'task2_instruction', 'task2_groundtruth']).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task2.write_parquet(data_dir / 'address_with_instructions.parquet', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
