{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polars version: 1.9.0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import string\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "print(f'polars version: {pl.__version__}')\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"geocoder_llm_project\", timeout=300)\n",
    "\n",
    "project_dir = Path(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://nationaladdressdata.s3.amazonaws.com/NAD_r18_TXT.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip_data_file_path = project_dir / 'NAD_r18_TXT.zip'\n",
    "\n",
    "# with zipfile.ZipFile(zip_data_file_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file_path = project_dir / 'TXT/NAD_r18.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset schema. Everything is a string here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_overrides = {\n",
    "    \"OID_\": pl.Int64,\n",
    "    \"AddNum_Pre\": pl.Utf8,\n",
    "    \"Add_Number\": pl.Int64,\n",
    "    \"AddNum_Suf\": pl.Utf8,\n",
    "    \"AddNo_Full\": pl.Int64,\n",
    "    \"St_PreMod\": pl.Utf8,\n",
    "    \"St_PreDir\": pl.Utf8,\n",
    "    \"St_PreTyp\": pl.Utf8,\n",
    "    \"St_PreSep\": pl.Utf8,\n",
    "    \"St_Name\": pl.Utf8,\n",
    "    \"St_PosTyp\": pl.Utf8,\n",
    "    \"St_PosDir\": pl.Utf8,\n",
    "    \"St_PosMod\": pl.Utf8,\n",
    "    \"StNam_Full\": pl.Utf8,\n",
    "    \"Building\": pl.Utf8,\n",
    "    \"Floor\": pl.Utf8,\n",
    "    \"Unit\": pl.Utf8,\n",
    "    \"Room\": pl.Utf8,\n",
    "    \"Seat\": pl.Utf8,\n",
    "    \"Addtl_Loc\": pl.Utf8,\n",
    "    \"SubAddress\": pl.Utf8,\n",
    "    \"LandmkName\": pl.Utf8,\n",
    "    \"County\": pl.Utf8,\n",
    "    \"Inc_Muni\": pl.Utf8,\n",
    "    \"Post_City\": pl.Utf8,\n",
    "    \"Census_Plc\": pl.Utf8,\n",
    "    \"Uninc_Comm\": pl.Utf8,\n",
    "    \"Nbrhd_Comm\": pl.Utf8,\n",
    "    \"NatAmArea\": pl.Utf8,\n",
    "    \"NatAmSub\": pl.Utf8,\n",
    "    \"Urbnztn_PR\": pl.Utf8,\n",
    "    \"PlaceOther\": pl.Utf8,\n",
    "    \"PlaceNmTyp\": pl.Utf8,\n",
    "    \"State\": pl.Utf8,\n",
    "    \"Zip_Code\": pl.Int64,\n",
    "    \"Plus_4\": pl.Int64,\n",
    "    \"UUID\": pl.Utf8,\n",
    "    \"AddAuth\": pl.Int64,\n",
    "    \"AddrRefSys\": pl.Utf8,\n",
    "    \"Longitude\": pl.Float64,\n",
    "    \"Latitude\": pl.Float64,\n",
    "    \"NatGrid\": pl.Utf8,\n",
    "    \"Elevation\": pl.Utf8,\n",
    "    \"Placement\": pl.Utf8,\n",
    "    \"AddrPoint\": pl.Utf8,\n",
    "    \"Related_ID\": pl.Utf8,\n",
    "    \"RelateType\": pl.Utf8,\n",
    "    \"ParcelSrc\": pl.Utf8,\n",
    "    \"Parcel_ID\": pl.Utf8,\n",
    "    \"AddrClass\": pl.Utf8,\n",
    "    \"Lifecycle\": pl.Utf8,\n",
    "    \"Effective\": pl.Utf8,\n",
    "    \"Expire\": pl.Utf8,\n",
    "    \"DateUpdate\": pl.Utf8,\n",
    "    \"AnomStatus\": pl.Utf8,\n",
    "    \"LocatnDesc\": pl.Utf8,\n",
    "    \"Addr_Type\": pl.Utf8,\n",
    "    \"DeliverTyp\": pl.Utf8,\n",
    "    \"NAD_Source\": pl.Utf8,\n",
    "    \"DataSet_ID\": pl.Utf8,\n",
    "    \"StreetAddress\": pl.Utf8,\n",
    "    \"SecondaryAddress\": pl.Utf8,\n",
    "    \"CityStateZip\": pl.Utf8,\n",
    "    \"FullAddress\": pl.Utf8,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data is huge (31GB) and build over years, there are some inconsistencies. So we ignore errors, infer schema and provide null values as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(\n",
    "    raw_file_path, \n",
    "    ignore_errors=True, \n",
    "    separator=\",\", \n",
    "    infer_schema_length=0, \n",
    "    quote_char=None, \n",
    "    schema_overrides=schema_overrides,\n",
    "    truncate_ragged_lines=True,\n",
    "    null_values=[\"Not stated\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out states which are not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(pl.col('State').is_not_null())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the states available in the data. We have data from 47 states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_states = [\n",
    "    'TX', 'LA', 'ME', 'WY', 'KY', 'MI', 'WA', 'VT', 'ND', 'TN',\n",
    "    'IN', 'WV', 'MN', 'RI', 'DE', 'IL', 'SD', 'AK', 'MS', 'OK',\n",
    "    'PA', 'WI', 'NY', 'KS', 'NM', 'AZ', 'SC', 'FL', 'NC', 'MD',\n",
    "    'UT', 'NE', 'NH', 'VA', 'GA', 'AL', 'CA', 'MA', 'CT', 'AR',\n",
    "    'CO', 'MT', 'DC', 'ID', 'IA', 'OH', 'MO'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(\n",
    "    pl.col('State').is_in(valid_states)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 80044721\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of records: {len(df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatinating different columns into single strings to get street and country information, and finally build the FullAddress column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(\n",
    "    pl.concat_str(\n",
    "        [\n",
    "            pl.col(\"AddNum_Pre\"),\n",
    "            pl.col(\"Add_Number\").cast(str),\n",
    "            pl.col(\"AddNum_Suf\"),\n",
    "            pl.col(\"St_PreMod\"),\n",
    "            pl.col(\"St_PreDir\"),\n",
    "            pl.col(\"St_PreTyp\"),\n",
    "            pl.col(\"St_Name\"),\n",
    "            pl.col(\"St_PosTyp\"),\n",
    "            pl.col(\"St_PosDir\"),\n",
    "            pl.col(\"St_PosMod\"),\n",
    "        ],\n",
    "        separator=\" \",\n",
    "        ignore_nulls=True\n",
    "    ).alias(\"StreetAddress\"),\n",
    "    pl.concat_str(\n",
    "        [\n",
    "            pl.col(\"Building\"),\n",
    "            pl.col(\"Floor\"),\n",
    "            pl.col(\"Unit\"),\n",
    "            pl.col(\"Room\"),\n",
    "            pl.col(\"Seat\"),\n",
    "            pl.col(\"Addtl_Loc\"),\n",
    "            pl.col(\"SubAddress\")\n",
    "        ],\n",
    "        separator=\", \",\n",
    "        ignore_nulls=True\n",
    "    ).alias(\"SecondaryAddress\"),\n",
    "    pl.concat_str(\n",
    "        [\n",
    "            pl.col(\"LandmkName\"),\n",
    "            pl.col(\"County\"),\n",
    "            pl.col(\"Inc_Muni\"),\n",
    "            pl.col(\"Post_City\"),\n",
    "            pl.col(\"State\"),\n",
    "            pl.concat_str(\n",
    "                [pl.col(\"Zip_Code\").cast(str), pl.col(\"Plus_4\").cast(str)],\n",
    "                separator=\"-\",\n",
    "                ignore_nulls=True\n",
    "            )\n",
    "        ],\n",
    "        separator=\", \",\n",
    "        ignore_nulls=True\n",
    "    ).alias(\"CityStateZip\")\n",
    ")\n",
    "\n",
    "df = df.with_columns(\n",
    "    pl.concat_str(\n",
    "        [\n",
    "            pl.col(\"StreetAddress\"),\n",
    "            pl.col(\"SecondaryAddress\"),\n",
    "            pl.col(\"CityStateZip\")\n",
    "        ],\n",
    "        separator=\"\\n\",\n",
    "        ignore_nulls=True\n",
    "    ).alias(\"FullAddress\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some inconsistencies found in the `Inc_Muni` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['UNKN', '250240201300', '510000105900', '**PREVIOUS NAME REMOVED BY FDC. (MAYBERRY COURT)', '631332003700', 'SWWJDU 15-5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Mh Sw' = 'south west', 'Mm 100.8 I95 Sb Hwy', 'Hwy' = 'highway', 'Lti' = '', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_chars = '?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "617 MEXBORO Road\n",
      "\n",
      "Monroe, UNINCORPORATED, FRISCO CITY, AL, 36445\n"
     ]
    }
   ],
   "source": [
    "idx = 10\n",
    "print(df['FullAddress'][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean & Median address length: 41-42 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Address Length: 42.41529813065374 | Median Address Length: 41.0\n"
     ]
    }
   ],
   "source": [
    "mean_add_len = df.with_columns(pl.col('FullAddress').str.len_chars().alias('AddressLength'))['AddressLength'].mean()\n",
    "median_add_len = df.with_columns(pl.col('FullAddress').str.len_chars().alias('AddressLength'))['AddressLength'].median()\n",
    "\n",
    "print(f'Mean Address Length: {mean_add_len} | Median Address Length: {median_add_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State full name and abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_name_abbr_tuples = [\n",
    "    (\"Alabama\", \"AL\"),\n",
    "    (\"Alaska\", \"AK\"),\n",
    "    (\"Arizona\", \"AZ\"),\n",
    "    (\"Arkansas\", \"AR\"),\n",
    "    (\"California\", \"CA\"),\n",
    "    (\"Colorado\", \"CO\"),\n",
    "    (\"Connecticut\", \"CT\"),\n",
    "    (\"Delaware\", \"DE\"),\n",
    "    (\"District of Columbia\", \"DC\"),\n",
    "    (\"Florida\", \"FL\"),\n",
    "    (\"Georgia\", \"GA\"),\n",
    "    (\"Idaho\", \"ID\"),\n",
    "    (\"Illinois\", \"IL\"),\n",
    "    (\"Indiana\", \"IN\"),\n",
    "    (\"Iowa\", \"IA\"),\n",
    "    (\"Kansas\", \"KS\"),\n",
    "    (\"Kentucky\", \"KY\"),\n",
    "    (\"Louisiana\", \"LA\"),\n",
    "    (\"Maine\", \"ME\"),\n",
    "    (\"Maryland\", \"MD\"),\n",
    "    (\"Massachusetts\", \"MA\"),\n",
    "    (\"Michigan\", \"MI\"),\n",
    "    (\"Minnesota\", \"MN\"),\n",
    "    (\"Mississippi\", \"MS\"),\n",
    "    (\"Missouri\", \"MO\"),\n",
    "    (\"Montana\", \"MT\"),\n",
    "    (\"Nebraska\", \"NE\"),\n",
    "    (\"New Hampshire\", \"NH\"),\n",
    "    (\"New Mexico\", \"NM\"),\n",
    "    (\"New York\", \"NY\"),\n",
    "    (\"North Carolina\", \"NC\"),\n",
    "    (\"North Dakota\", \"ND\"),\n",
    "    (\"Ohio\", \"OH\"),\n",
    "    (\"Oklahoma\", \"OK\"),\n",
    "    (\"Pennsylvania\", \"PA\"),\n",
    "    (\"Rhode Island\", \"RI\"),\n",
    "    (\"South Carolina\", \"SC\"),\n",
    "    (\"South Dakota\", \"SD\"),\n",
    "    (\"Tennessee\", \"TN\"),\n",
    "    (\"Texas\", \"TX\"),\n",
    "    (\"Utah\", \"UT\"),\n",
    "    (\"Vermont\", \"VT\"),\n",
    "    (\"Virginia\", \"VA\"),\n",
    "    (\"Washington\", \"WA\"),\n",
    "    (\"West Virginia\", \"WV\"),\n",
    "    (\"Wisconsin\", \"WI\"),\n",
    "    (\"Wyoming\", \"WY\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling 10K addresses for each state, from the entire dataset. We do sampling without replacement as we don't want duplicates in the dataset. So we have a dataset of 470K records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_per_state = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_df(df: pl.DataFrame, state_abv: str, samples: int = address_per_state) -> pl.DataFrame:\n",
    "    state_df = df.filter(pl.col('State') == state_abv)\n",
    "\n",
    "    sample_with_replacement = True if len(state_df) < samples else False\n",
    "\n",
    "    return state_df.sample(n=samples, seed=0, with_replacement=sample_with_replacement, shuffle=True) \n",
    "\n",
    "def build_dataset(df: pl.DataFrame, states: List[Tuple[str, str]]) -> pl.DataFrame:\n",
    "    dfs = [get_state_df(df, state_abv) for state, state_abv in states]\n",
    "    return pl.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = build_dataset(df, state_name_abbr_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 470000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of samples: {len(sampled_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique states: 47\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of unique states: {len(sampled_df[\"State\"].unique())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the dataset into a parquet format because it is a columnar store and compresses efficiently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df.write_parquet(project_dir / 'nad_sample_address.parquet', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to Sequence Dataset prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = Path(os.getcwd()).parent\n",
    "data_dir = project_dir / 'Data'\n",
    "null_values = [\"unkn\", \"unincorporated\", \"unknown\", \"null\", \"nan\", \"null\", \"nill\", \"na\", \"none\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (470000, 64)\n",
      "States: ['SD', 'IL', 'VA', 'MD', 'MT', 'NM', 'AZ', 'NY', 'CA', 'WY', 'AL', 'TN', 'ND', 'MS', 'MA', 'WI', 'MI', 'KY', 'WA', 'VT', 'FL', 'ME', 'GA', 'MO', 'AK', 'OH', 'ID', 'OK', 'CO', 'CT', 'IA', 'IN', 'PA', 'RI', 'NC', 'NE', 'AR', 'DE', 'LA', 'KS', 'TX', 'DC', 'MN', 'UT', 'SC', 'WV', 'NH']\n"
     ]
    }
   ],
   "source": [
    "df = pl.read_parquet(data_dir / 'address_dataset.parquet')\n",
    "print(f'Shape of data: {df.shape}')\n",
    "print(f\"States: {df['State'].unique().to_list()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_mapping = {\n",
    "    'TX': 'Texas',\n",
    "    'LA': 'Louisiana',\n",
    "    'ME': 'Maine',\n",
    "    'WY': 'Wyoming',\n",
    "    'KY': 'Kentucky',\n",
    "    'MI': 'Michigan',\n",
    "    'WA': 'Washington',\n",
    "    'VT': 'Vermont',\n",
    "    'ND': 'North Dakota',\n",
    "    'TN': 'Tennessee',\n",
    "    'IN': 'Indiana',\n",
    "    'WV': 'West Virginia',\n",
    "    'MN': 'Minnesota',\n",
    "    'RI': 'Rhode Island',\n",
    "    'DE': 'Delaware',\n",
    "    'IL': 'Illinois',\n",
    "    'SD': 'South Dakota',\n",
    "    'AK': 'Alaska',\n",
    "    'MS': 'Mississippi',\n",
    "    'OK': 'Oklahoma',\n",
    "    'PA': 'Pennsylvania',\n",
    "    'WI': 'Wisconsin',\n",
    "    'NY': 'New York',\n",
    "    'KS': 'Kansas',\n",
    "    'NM': 'New Mexico',\n",
    "    'AZ': 'Arizona',\n",
    "    'SC': 'South Carolina',\n",
    "    'FL': 'Florida',\n",
    "    'NC': 'North Carolina',\n",
    "    'MD': 'Maryland',\n",
    "    'UT': 'Utah',\n",
    "    'NE': 'Nebraska',\n",
    "    'NH': 'New Hampshire',\n",
    "    'VA': 'Virginia',\n",
    "    'GA': 'Georgia',\n",
    "    'AL': 'Alabama',\n",
    "    'CA': 'California',\n",
    "    'MA': 'Massachusetts',\n",
    "    'CT': 'Connecticut',\n",
    "    'AR': 'Arkansas',\n",
    "    'CO': 'Colorado',\n",
    "    'MT': 'Montana',\n",
    "    'DC': 'District of Columbia',\n",
    "    'ID': 'Idaho',\n",
    "    'IA': 'Iowa',\n",
    "    'OH': 'Ohio',\n",
    "    'MO': 'Missouri'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lower case all the string columns, and then replace the occurances of null value strings with np.nan and the replace them with empty strings ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.select(pl.col(pl.Utf8)).columns:\n",
    "    df = df.with_columns(\n",
    "        pl.col(column).str.to_lowercase().alias(column)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.select(pl.col(pl.Utf8)).columns:\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col(column).is_in(null_values)).then(np.nan).otherwise(pl.col(column)).alias(column)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fill_null('')\n",
    "df = df.fill_nan('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.select(pl.col(pl.Utf8)).columns:\n",
    "    x = df.filter(\n",
    "        pl.col(column) == 'nan'\n",
    "    )\n",
    "    if len(x) > 0:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to build the full address and format it appropriately. I try to follow the address format from geopy's `Nominatim` class. This class connects with the openstreet maps data and provides the latitude, longitude and the full address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_usdot_to_freeform_granular(data: dict, state_map: dict) -> str:\n",
    "    # Custom null-like values to filter\n",
    "    NULL_STRINGS = {\"\", None, \"nan\", \"null\"}\n",
    "\n",
    "    def safe_get(key):\n",
    "        val = data.get(key)\n",
    "        if isinstance(val, str):\n",
    "            val = val.lower()\n",
    "        return None if (val in NULL_STRINGS or str(val).strip() in NULL_STRINGS) else str(val).strip()\n",
    "\n",
    "    def safe_title(key):\n",
    "        val = safe_get(key)\n",
    "        return val.title() if val else None\n",
    "\n",
    "    # House number\n",
    "    number = \" \".join(filter(None, [safe_get(\"AddNum_Pre\"),\n",
    "                                    safe_get(\"Add_Number\"),\n",
    "                                    safe_get(\"AddNum_Suf\")]))\n",
    "\n",
    "    # Street full\n",
    "    street_parts = [\n",
    "        safe_get(\"St_PreDir\"),\n",
    "        safe_title(\"St_Name\"),\n",
    "        safe_title(\"St_PosTyp\"),\n",
    "        safe_get(\"St_PosDir\")\n",
    "    ]\n",
    "    street = \" \".join(part for part in street_parts if part)\n",
    "\n",
    "    # Unit/building details\n",
    "    sub_parts = []\n",
    "    if safe_get(\"Building\"): sub_parts.append(f\"Bldg {safe_get('Building')}\")\n",
    "    if safe_get(\"Floor\"): sub_parts.append(f\"Floor {safe_get('Floor')}\")\n",
    "    if safe_get(\"Unit\"): sub_parts.append(f\"Unit {safe_get('Unit')}\")\n",
    "    if safe_get(\"Room\"): sub_parts.append(f\"Room {safe_get('Room')}\")\n",
    "\n",
    "    sub_address = \", \".join(sub_parts)\n",
    "\n",
    "    # Town/City\n",
    "    town = safe_title(\"Uninc_Comm\") or safe_title(\"Inc_Muni\")\n",
    "\n",
    "    # County\n",
    "    county = safe_title(\"County\")\n",
    "\n",
    "    # State\n",
    "    state_abbr = safe_get(\"State\")\n",
    "    state_full = state_map.get(state_abbr.upper(), state_abbr) if state_abbr else None\n",
    "\n",
    "    # ZIP\n",
    "    zip_raw = safe_get(\"Zip_Code\")\n",
    "    zip_code = zip_raw.zfill(5) if zip_raw and zip_raw.isdigit() else None\n",
    "\n",
    "    # Compose full address\n",
    "    components = [number, street]\n",
    "    if sub_address:\n",
    "        components.append(sub_address)\n",
    "    components.extend([\n",
    "        town,\n",
    "        f\"{county} County\" if county else None,\n",
    "        state_full,\n",
    "        zip_code\n",
    "    ])\n",
    "\n",
    "    return \", \".join([c for c in components if c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_addresses = [\n",
    "    format_usdot_to_freeform_granular(r, state_mapping) \n",
    "    for r in df.rows(named=True)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(\n",
    "    pl.Series(\"FormattedFullAddress\", formatted_addresses)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean and median address lengths have increased to 65 characters now. This new formatting makes the address strings more clear and easier to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Address Length: 64.9411170212766 | Median Address Length: 64.0\n"
     ]
    }
   ],
   "source": [
    "mean_add_len = df.with_columns(pl.col('FormattedFullAddress').str.len_chars().alias('AddressLength'))['AddressLength'].mean()\n",
    "median_add_len = df.with_columns(pl.col('FormattedFullAddress').str.len_chars().alias('AddressLength'))['AddressLength'].median()\n",
    "\n",
    "print(f'Mean Address Length: {mean_add_len} | Median Address Length: {median_add_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OID_': [72099617],\n",
       " 'FullAddress': ['472 south main street\\n\\ncamp hill, al, 36850'],\n",
       " 'FormattedFullAddress': ['472, south Main Street, Camp Hill, Tallapoosa County, Alabama, 36850'],\n",
       " 'Latitude': ['32.79596540137694'],\n",
       " 'Longitude': ['-85.6535596907001']}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "df.select(['OID_', 'FullAddress', 'FormattedFullAddress', 'Latitude', 'Longitude'])[idx].to_dict(as_series=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validating the Formatted Full Address with the Full Address from Nominatim using reverse geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13, Daniels Place, Dixwell, New Haven, Connecticut, 06511, United States\n",
      "41.3177839 -72.932018\n"
     ]
    }
   ],
   "source": [
    "# location = geolocator.geocode(\"13, John Daniels Place, New Haven County, Connecticut, 06511\")\n",
    "# location = geolocator.geocode({\"postalcode\": int(\"06511\"), \"country\": \"US\"})\n",
    "location = geolocator.reverse(['41.317783902424', '-72.9320178229665'])\n",
    "\n",
    "if location is None:\n",
    "    print(\"Location not found.\")\n",
    "else:\n",
    "    print(location.address)\n",
    "    print(location.latitude, location.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4524, Old Caldwell Mill Road, Shelby County, Alabama, 35242\n",
      "33.41236637208968 -86.73952124099591\n"
     ]
    }
   ],
   "source": [
    "idx = 100\n",
    "print(df[idx]['FormattedFullAddress'].item())\n",
    "print(df[idx]['Latitude'].item(), df[idx]['Longitude'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write_parquet(data_dir / 'new_formatted_addresses.parquet', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the newly formatted address matches more closely with the standard open street maps address."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now building the source-target pairs for supervised fine tuning. The source is the unnormalized / address with mistakes and target is the cleaned address. \n",
    "To generate noisy source addresses, we inject the following noise: <br>\n",
    " Noise Types Introduced\n",
    "\n",
    "| Noise Type              | Field           | Description                                                                 |\n",
    "|-------------------------|------------------|-----------------------------------------------------------------------------|\n",
    "| Street Number Removal   | `Add_Number`     | 50% chance to remove the house/building number (`None`)                    |\n",
    "| Character Corruption    | `St_Name`        | 20% per character: replace characters randomly (simulating typos)          |\n",
    "| City Dropping           | `Post_City`      | 30% chance to remove city field                                             |\n",
    "| ZIP Code Truncation     | `Zip_Code`       | 20% chance to truncate ZIP (e.g., `36078` → `3607`)                         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clean_address(row: Dict) -> str:\n",
    "    \"\"\"Construct address from structured fields with type consistency\"\"\"\n",
    "    parts = []\n",
    "    \n",
    "    # Street component\n",
    "    street = []\n",
    "    if row.get('Add_Number') is not None:\n",
    "        street.append(str(int(row['Add_Number'])))\n",
    "    if row.get('St_Name'):\n",
    "        street.append(str(row['St_Name']).lower())\n",
    "    if street:\n",
    "        parts.append(' '.join(street))\n",
    "    \n",
    "    # Location component\n",
    "    location = []\n",
    "    if row.get('Post_City'):\n",
    "        location.append(str(row['Post_City']).lower())\n",
    "    if row.get('State'):\n",
    "        location.append(str(row['State']).lower())\n",
    "    if row.get('Zip_Code') is not None:\n",
    "        location.append(f\"{int(row['Zip_Code']):05d}\"[:5])\n",
    "    if location:\n",
    "        parts.append(', '.join(location))\n",
    "    \n",
    "    return ', '.join(parts)\n",
    "\n",
    "def add_character_noise(component: str) -> str:\n",
    "    \"\"\"Add character noise while maintaining string type\"\"\"\n",
    "    return ''.join([\n",
    "        random.choice(string.ascii_lowercase) \n",
    "        if c.isalpha() and random.random() < 0.2 \n",
    "        else c\n",
    "        for c in component\n",
    "    ]) if component else component\n",
    "\n",
    "def generate_noisy_address(row: Dict) -> str:\n",
    "    \"\"\"Generate noisy address with type-safe modifications\"\"\"\n",
    "    modified = row.copy()\n",
    "    \n",
    "    # 50% chance to remove street number (set to None)\n",
    "    if random.random() < 0.5:\n",
    "        modified['Add_Number'] = None\n",
    "    \n",
    "    # Add noise to street name (keep as string)\n",
    "    if modified.get('St_Name'):\n",
    "        modified['St_Name'] = add_character_noise(str(modified['St_Name']))\n",
    "    \n",
    "    # 30% chance to remove city (set to None)\n",
    "    if random.random() < 0.3:\n",
    "        modified['Post_City'] = None\n",
    "    \n",
    "    # 20% chance to modify zip code (keep as integer)\n",
    "    if modified.get('Zip_Code') and random.random() < 0.2:\n",
    "        zip_code = int(modified['Zip_Code'])\n",
    "        if 10000 <= zip_code <= 99999:\n",
    "            modified['Zip_Code'] = zip_code // 10  # Truncate last digit\n",
    "    \n",
    "    return build_clean_address(modified)\n",
    "\n",
    "def create_address_pairs(df: pl.DataFrame, n_noisy_varient_per_add: int = 3) -> pl.DataFrame:\n",
    "    \"\"\"Generate address pairs with schema consistency\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for row in df.to_dicts():\n",
    "        oid = row['OID_']\n",
    "        state = row['State']\n",
    "\n",
    "        # Original clean target\n",
    "        clean_target = build_clean_address(row)\n",
    "        \n",
    "        # Add clean pair\n",
    "        results.append({\n",
    "            'oid': oid,\n",
    "            'source': clean_target,\n",
    "            'target': clean_target,\n",
    "            'state': state\n",
    "        })\n",
    "        \n",
    "        # Generate n noisy variants\n",
    "        for _ in range(n_noisy_varient_per_add):\n",
    "            noisy_source = generate_noisy_address(row)\n",
    "            results.append({\n",
    "                'oid': oid,\n",
    "                'source': noisy_source,\n",
    "                'target': clean_target,\n",
    "                'state': state\n",
    "            })\n",
    "    # Ensure schema consistency\n",
    "    return pl.DataFrame(results).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
